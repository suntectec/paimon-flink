env {
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 10000
}

source {
  Kafka {
    topic = "order_protobuf_format"
    format = protobuf
    protobuf_message_name = Order
    protobuf_schema = """
      syntax = "proto3";

      package org.apache.seatunnel.format.protobuf;

      option java_outer_classname = "ProtobufE2E";

      message Order {
        int64 id = 1;
        string order_id = 2;
        int32 supplier_id = 3;
        int32 item_id = 4;
        string status = 5;
        int32 qty = 6;
        int32 net_price = 7;
        string issued_at = 8;
        string completed_at = 9;
        string spec = 10;
        string created_at = 11;
        string updated_at = 12;
      }
    """
    schema = {
      fields {
        id = long
        order_id = string
        supplier_id = int
        item_id = int
        status = string
        qty = int
        net_price = int
        issued_at = string
        completed_at = string
        spec = string
        created_at = string
        updated_at = string
        }
    }
    bootstrap.servers = "192.168.138.15:9092"
    start_mode = "earliest"
    consumer.group = "seatunnel_group_1"
    format_error_handle_way = skip
  }
}

sink {
  Paimon {
    warehouse = "s3a://warehouse/paimon/seatunnel/"
    database="paimon"
    table="${table_name}"
    paimon.hadoop.conf = {
      fs.s3a.access-key=minioadmin
      fs.s3a.secret-key=minioadmin
      fs.s3a.endpoint="http://192.168.138.15:9000"
      fs.s3a.path.style.access=true
      fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    }
    paimon.table.write-props = {
      bucket = -1
      dynamic-bucket.target-row-num = 50000
    }
    paimon.table.primary-keys = "id"
  }
}